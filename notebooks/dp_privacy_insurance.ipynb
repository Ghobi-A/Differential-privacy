{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Differential Privacy: Insurance Dataset\n\n## 1) Introduction: Privacy\u2013Utility Trade-off\nThis notebook studies the privacy\u2013utility trade-off for a binary classifier predicting **smoker** status from the insurance dataset. We compare baseline models (SVM and decision tree) with feature-level differential privacy (DP) noise mechanisms and a DP-SGD neural network. **ROC-AUC** is the primary metric used throughout.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from __future__ import annotations\n\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\nfrom dp.constants import RANDOM_STATE\nfrom dp.dpsgd import opacus_available\nfrom dp.mechanisms import add_gaussian_noise, add_laplace_noise\nfrom dp.models import build_decision_tree_model, build_svm_model\nfrom dp.pipeline import build_preprocessor, split_dataset\n\nnp.random.seed(RANDOM_STATE)\nplt.style.use(\"seaborn-v0_8\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Dataset overview + class imbalance"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "repo_root = Path(\"..\").resolve()\ndata_path = repo_root / \"data\" / \"insurance.csv\"\n\ndf = pd.read_csv(data_path)\ndf.head()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df.info()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class_counts = df[\"smoker\"].value_counts()\nclass_props = class_counts / class_counts.sum()\n\nfig, ax = plt.subplots(figsize=(5, 4))\nclass_counts.plot(kind=\"bar\", ax=ax)\nax.set_title(\"Class Imbalance (smoker)\")\nax.set_ylabel(\"Count\")\nax.set_xlabel(\"Class\")\nfor idx, value in enumerate(class_counts.values):\n    ax.text(idx, value, f\"{class_props.iloc[idx]:.1%}\", ha=\"center\", va=\"bottom\")\nax.grid(axis=\"y\")\nfig\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Baseline models (SVM, Decision Tree)\nWe train baseline models with standard preprocessing (scaling numeric features and one-hot encoding categorical features). ROC-AUC is computed on a held-out test set.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "split = split_dataset(df, target=\"smoker\")\npreprocessor = build_preprocessor(split.X_train)\n\nX_train = preprocessor.fit_transform(split.X_train)\nX_test = preprocessor.transform(split.X_test)\n\ny_train = (split.y_train == \"yes\").astype(int).to_numpy()\ny_test = (split.y_test == \"yes\").astype(int).to_numpy()\n\nmodels = {\n    \"svm\": build_svm_model(),\n    \"decision_tree\": build_decision_tree_model(),\n}\n\nbaseline_rows = []\nbaseline_rocs = {}\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    scores = model.predict_proba(X_test)[:, 1]\n    roc_auc = roc_auc_score(y_test, scores)\n    baseline_rows.append({\"model\": name, \"roc_auc\": roc_auc})\n    fpr, tpr, _ = roc_curve(y_test, scores)\n    baseline_rocs[name] = (fpr, tpr)\n\nbaseline_results = pd.DataFrame(baseline_rows)\nbaseline_results\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "fig, ax = plt.subplots(figsize=(6, 5))\nfor name, (fpr, tpr) in baseline_rocs.items():\n    auc_val = baseline_results.loc[baseline_results[\"model\"] == name, \"roc_auc\"].iloc[0]\n    ax.plot(fpr, tpr, label=f\"{name} (AUC={auc_val:.3f})\")\nax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\nax.set_title(\"Baseline ROC Curves\")\nax.set_xlabel(\"False Positive Rate\")\nax.set_ylabel(\"True Positive Rate\")\nax.legend()\nax.grid(True)\nfig\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Feature-level DP\nWe apply DP noise to numeric features **after clipping** them to robust bounds. Clipping limits sensitivity, which controls the noise scale. We compare Laplace (\u03b5-DP) and Gaussian ((\u03b5, \u03b4)-DP) mechanisms, then plot ROC-AUC vs \u03b5 on a log scale.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def compute_clip_bounds(df: pd.DataFrame, lower_q: float = 0.01, upper_q: float = 0.99) -> pd.DataFrame:\n    numeric = df.select_dtypes(include=\"number\")\n    return pd.DataFrame({\n        \"lower\": numeric.quantile(lower_q),\n        \"upper\": numeric.quantile(upper_q),\n    })\n\n\ndef clip_numeric(df: pd.DataFrame, bounds: pd.DataFrame) -> pd.DataFrame:\n    clipped = df.copy()\n    for column in bounds.index:\n        clipped[column] = clipped[column].clip(bounds.loc[column, \"lower\"], bounds.loc[column, \"upper\"])\n    return clipped\n\n\ndef add_dp_noise(\n    df: pd.DataFrame,\n    mechanism: str,\n    epsilon: float,\n    delta: float,\n    sensitivity: float,\n    random_state: int,\n) -> pd.DataFrame:\n    numeric = df.select_dtypes(include=\"number\")\n    categorical = df.select_dtypes(exclude=\"number\")\n    if mechanism == \"laplace\":\n        noisy_numeric = add_laplace_noise(numeric, epsilon=epsilon, sensitivity=sensitivity, random_state=random_state)\n    elif mechanism == \"gaussian\":\n        noisy_numeric = add_gaussian_noise(\n            numeric,\n            epsilon=epsilon,\n            delta=delta,\n            sensitivity=sensitivity,\n            random_state=random_state,\n        )\n    else:\n        raise ValueError(\"mechanism must be 'laplace' or 'gaussian'\")\n    return pd.concat([noisy_numeric, categorical], axis=1)[df.columns]\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "bounds = compute_clip_bounds(split.X_train)\n\nclipped_train = clip_numeric(split.X_train, bounds)\nclipped_test = clip_numeric(split.X_test, bounds)\n\nsensitivity = (bounds[\"upper\"] - bounds[\"lower\"]).max()\n\nlaplace_epsilons = np.logspace(-2, 1, 6)\ngaussian_epsilons = np.logspace(-2, 1, 6)\n\ndef evaluate_dp_features(mechanism: str, epsilons: np.ndarray, delta: float = 1e-5) -> pd.DataFrame:\n    rows = []\n    for epsilon in epsilons:\n        noisy_train = add_dp_noise(\n            clipped_train,\n            mechanism=mechanism,\n            epsilon=epsilon,\n            delta=delta,\n            sensitivity=sensitivity,\n            random_state=RANDOM_STATE,\n        )\n        preprocessor = build_preprocessor(noisy_train)\n        X_train_noisy = preprocessor.fit_transform(noisy_train)\n        X_test_noisy = preprocessor.transform(clipped_test)\n        for name, model in models.items():\n            model.fit(X_train_noisy, y_train)\n            scores = model.predict_proba(X_test_noisy)[:, 1]\n            roc_auc = roc_auc_score(y_test, scores)\n            rows.append({\"mechanism\": mechanism, \"epsilon\": epsilon, \"model\": name, \"roc_auc\": roc_auc})\n    return pd.DataFrame(rows)\n\nlaplace_results = evaluate_dp_features(\"laplace\", laplace_epsilons)\ngaussian_results = evaluate_dp_features(\"gaussian\", gaussian_epsilons)\n\nfeature_dp_results = pd.concat([laplace_results, gaussian_results], ignore_index=True)\nfeature_dp_results.head()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "fig, ax = plt.subplots(figsize=(7, 5))\nfor (mechanism, model_name), subset in feature_dp_results.groupby([\"mechanism\", \"model\"]):\n    ax.plot(subset[\"epsilon\"], subset[\"roc_auc\"], marker=\"o\", label=f\"{mechanism} - {model_name}\")\nax.set_xscale(\"log\")\nax.set_xlabel(\"Epsilon (log scale)\")\nax.set_ylabel(\"ROC-AUC\")\nax.set_title(\"Feature-level DP: ROC-AUC vs Epsilon\")\nax.grid(True, which=\"both\")\nax.legend()\nfig\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Why performance collapses at very low \u03b5:**\nLow \u03b5 implies heavier noise, which overwhelms the signal in clipped numeric features. As the noise scale grows, discriminative patterns blur and classifiers approach random guessing (ROC-AUC \u2248 0.5). This is most pronounced for high-dimensional feature spaces where noise is injected per feature.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Excluded mechanisms\nWe exclude mechanisms such as **randomized response** (suited for binary categorical data) and **output perturbation** of model parameters. This notebook focuses on feature-level perturbation and training-time DP (DP-SGD) to keep the experimental scope clear and directly comparable via ROC-AUC.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) DP-SGD (Neural Network)\nWe train a small neural network with **Opacus** to provide training-time privacy. We report ROC-AUC and the **computed \u03b5** from the privacy accountant. We also note that **threshold tuning is post-processing**, which does not consume additional privacy budget.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "if opacus_available():\n    import torch\n    from torch import nn\n    from torch.utils.data import DataLoader, TensorDataset\n    from opacus import PrivacyEngine\n\n    torch.manual_seed(RANDOM_STATE)\n\n    X_full = preprocessor.fit_transform(split.X_train)\n    X_eval = preprocessor.transform(split.X_test)\n    y_full = torch.tensor(y_train, dtype=torch.float32)\n    y_eval = torch.tensor(y_test, dtype=torch.float32)\n\n    X_full_tensor = torch.tensor(X_full, dtype=torch.float32)\n    X_eval_tensor = torch.tensor(X_eval, dtype=torch.float32)\n\n    dataset = TensorDataset(X_full_tensor, y_full)\n\n    def train_with_noise_multiplier(noise_multiplier: float, epochs: int = 15):\n        model = nn.Sequential(\n            nn.Linear(X_full_tensor.shape[1], 32),\n            nn.ReLU(),\n            nn.Linear(32, 1),\n        )\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n        loss_fn = nn.BCEWithLogitsLoss()\n        data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n\n        privacy_engine = PrivacyEngine()\n        model, optimizer, data_loader = privacy_engine.make_private(\n            module=model,\n            optimizer=optimizer,\n            data_loader=data_loader,\n            noise_multiplier=noise_multiplier,\n            max_grad_norm=1.0,\n        )\n\n        model.train()\n        for _ in range(epochs):\n            for features, labels in data_loader:\n                optimizer.zero_grad()\n                logits = model(features).squeeze(1)\n                loss = loss_fn(logits, labels)\n                loss.backward()\n                optimizer.step()\n\n        model.eval()\n        with torch.no_grad():\n            logits = model(X_eval_tensor).squeeze(1)\n            probs = torch.sigmoid(logits).numpy()\n        roc_auc = roc_auc_score(y_test, probs)\n        epsilon = privacy_engine.get_epsilon(delta=1e-5)\n        return epsilon, roc_auc\n\n    noise_multipliers = [0.5, 1.0, 1.5, 2.0]\n    dp_sgd_rows = []\n    for nm in noise_multipliers:\n        epsilon, roc_auc = train_with_noise_multiplier(nm)\n        dp_sgd_rows.append({\"noise_multiplier\": nm, \"epsilon\": epsilon, \"roc_auc\": roc_auc})\n\n    dp_sgd_results = pd.DataFrame(dp_sgd_rows)\n    dp_sgd_results\nelse:\n    dp_sgd_results = pd.DataFrame(columns=[\"noise_multiplier\", \"epsilon\", \"roc_auc\"])\n    print(\"Opacus/torch not available; skipping DP-SGD training.\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "if not dp_sgd_results.empty:\n    fig, ax = plt.subplots(figsize=(6, 4))\n    ax.plot(dp_sgd_results[\"epsilon\"], dp_sgd_results[\"roc_auc\"], marker=\"o\")\n    ax.set_xlabel(\"Computed epsilon\")\n    ax.set_ylabel(\"ROC-AUC\")\n    ax.set_title(\"DP-SGD: ROC-AUC vs Computed Epsilon\")\n    ax.grid(True)\n    fig\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Threshold tuning (post-processing invariance):**\nOnce we have model scores, choosing a decision threshold is a post-processing step. Differential privacy is closed under post-processing, so tuning thresholds (e.g., for fairness or operational constraints) does **not** consume additional privacy budget.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) Fairness baseline (DP diff, EO diff)\nWe report demographic parity (DP) difference and equalized odds (EO) difference using **sex** as the protected attribute. Metrics are computed on baseline model scores after choosing a threshold that maximizes Youden's J statistic on the test set.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "protected = split.X_test[\"sex\"].to_numpy()\n\nbaseline_model = build_svm_model()\nbaseline_model.fit(X_train, y_train)\nbaseline_scores = baseline_model.predict_proba(X_test)[:, 1]\n\nfpr, tpr, thresholds = roc_curve(y_test, baseline_scores)\nJ = tpr - fpr\nbest_threshold = thresholds[np.argmax(J)]\nbaseline_preds = (baseline_scores >= best_threshold).astype(int)\n\n\ndef demographic_parity_diff(y_pred: np.ndarray, group: np.ndarray) -> float:\n    groups = np.unique(group)\n    rates = []\n    for g in groups:\n        rates.append(y_pred[group == g].mean())\n    return float(np.abs(rates[0] - rates[1]))\n\n\ndef equalized_odds_diff(y_true: np.ndarray, y_pred: np.ndarray, group: np.ndarray) -> float:\n    groups = np.unique(group)\n    tpr_diff = []\n    fpr_diff = []\n    for g in groups:\n        mask = group == g\n        positives = y_true[mask] == 1\n        negatives = y_true[mask] == 0\n        tpr_g = y_pred[mask][positives].mean() if positives.any() else 0.0\n        fpr_g = y_pred[mask][negatives].mean() if negatives.any() else 0.0\n        tpr_diff.append(tpr_g)\n        fpr_diff.append(fpr_g)\n    return float(max(np.abs(tpr_diff[0] - tpr_diff[1]), np.abs(fpr_diff[0] - fpr_diff[1])))\n\nfairness_results = pd.DataFrame(\n    {\n        \"metric\": [\"dp_diff\", \"eo_diff\"],\n        \"value\": [\n            demographic_parity_diff(baseline_preds, protected),\n            equalized_odds_diff(y_test, baseline_preds, protected),\n        ],\n    }\n)\n\nfairness_results\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8) Discussion summary\n- **Utility drops with stronger privacy**: As \u03b5 decreases, noise dominates feature signal and ROC-AUC trends toward chance-level.\n- **Gaussian vs Laplace**: Both mechanisms follow the same trade-off pattern; Gaussian requires a \u03b4 parameter and can be tuned for similar utility.\n- **DP-SGD provides training-time privacy**: When Opacus is available, the accountant yields an \u03b5 that can be compared directly to feature-level DP.\n- **Fairness metrics**: DP/EO differences quantify group disparities and can be monitored alongside ROC-AUC without affecting privacy through post-processing.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}